{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약할 텍스트 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77669</th>\n",
       "      <td>Lalu Prasad calls Nitish Kumar 'palturam' of I...</td>\n",
       "      <td>Talking about Bihar CM Nitish Kumar, RJD chief...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89894</th>\n",
       "      <td>United made a terrible mistake in dragging fli...</td>\n",
       "      <td>Billionaire Warren Buffett, the top investor i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10353</th>\n",
       "      <td>4 minor girls flee Bihar shelter home using du...</td>\n",
       "      <td>The police on Sunday said that four minor girl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14842</th>\n",
       "      <td>Scientists 3D-print cement paste that gets str...</td>\n",
       "      <td>Researchers at USA's Purdue University have 3D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82880</th>\n",
       "      <td>First look poster of Sidharth, Manoj's 'Aiyaar...</td>\n",
       "      <td>The first look poster of the Sidharth Malhotra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36829</th>\n",
       "      <td>Virgin Galactic finishes 1st powered flight si...</td>\n",
       "      <td>Richard Branson-led commercial spaceflight Vir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7343</th>\n",
       "      <td>UpGrad announces scholarship worth Ã¢ÂÂ¹5 Cr ...</td>\n",
       "      <td>UpGrad has announced a scholarship worth Ã¢ÂÂ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37657</th>\n",
       "      <td>Xiaomi cuts valuation to $55-70 billion ahead ...</td>\n",
       "      <td>Chinese smartphone maker Xiaomi has reportedly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57603</th>\n",
       "      <td>BHU exam paper asks to explain GST in Kautilya...</td>\n",
       "      <td>Students in Banaras Hindu University were aske...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16320</th>\n",
       "      <td>1 in 20 deaths worldwide caused due to alcohol...</td>\n",
       "      <td>Alcohol is responsible for over three million ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "77669  Lalu Prasad calls Nitish Kumar 'palturam' of I...   \n",
       "89894  United made a terrible mistake in dragging fli...   \n",
       "10353  4 minor girls flee Bihar shelter home using du...   \n",
       "14842  Scientists 3D-print cement paste that gets str...   \n",
       "82880  First look poster of Sidharth, Manoj's 'Aiyaar...   \n",
       "36829  Virgin Galactic finishes 1st powered flight si...   \n",
       "7343   UpGrad announces scholarship worth Ã¢ÂÂ¹5 Cr ...   \n",
       "37657  Xiaomi cuts valuation to $55-70 billion ahead ...   \n",
       "57603  BHU exam paper asks to explain GST in Kautilya...   \n",
       "16320  1 in 20 deaths worldwide caused due to alcohol...   \n",
       "\n",
       "                                                    text  \n",
       "77669  Talking about Bihar CM Nitish Kumar, RJD chief...  \n",
       "89894  Billionaire Warren Buffett, the top investor i...  \n",
       "10353  The police on Sunday said that four minor girl...  \n",
       "14842  Researchers at USA's Purdue University have 3D...  \n",
       "82880  The first look poster of the Sidharth Malhotra...  \n",
       "36829  Richard Branson-led commercial spaceflight Vir...  \n",
       "7343   UpGrad has announced a scholarship worth Ã¢ÂÂ...  \n",
       "37657  Chinese smartphone maker Xiaomi has reportedly...  \n",
       "57603  Students in Banaras Hindu University were aske...  \n",
       "16320  Alcohol is responsible for over three million ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n",
    "\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추출적 요약 Extractive Summaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines :  upGrad learner switches to career in ML & Al with 90% salary hike\n",
      "extractive summarization :  upGrad's Online Power Learning has powered 3 lakh+ careers.\n"
     ]
    }
   ],
   "source": [
    "from summa.summarizer import summarize\n",
    "\n",
    "print(\"headlines : \",data.iloc[0, 0])\n",
    "print(\"extractive summarization : \", summarize(data.iloc[0, 1], ratio=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        upGrad's Online Power Learning has powered 3 l...\n",
       "1        Users get one CRED coin per rupee of bill paid...\n",
       "2        The match witnessed India getting all out for ...\n",
       "3        Also, customers have options to insure against...\n",
       "4        Speaking about the sexual harassment allegatio...\n",
       "                               ...                        \n",
       "98396    A CRPF jawan was on Tuesday axed to death with...\n",
       "98397    'Uff Yeh', the first song from the Sonakshi Si...\n",
       "98398    Michael B Jordan will reportedly play the lead...\n",
       "98399    The video also shows a TV airing a news confer...\n",
       "98400                                                     \n",
       "Name: extractive_summary, Length: 98401, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ext = data[\"text\"].apply(lambda x : summarize(x, ratio=0.4))\n",
    "res_ext = res_ext.rename(\"extractive_summary\")\n",
    "res_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>extractive_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "      <td>upGrad's Online Power Learning has powered 3 l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "      <td>Users get one CRED coin per rupee of bill paid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "      <td>The match witnessed India getting all out for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "      <td>Also, customers have options to insure against...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98396</th>\n",
       "      <td>CRPF jawan axed to death by Maoists in Chhatti...</td>\n",
       "      <td>A CRPF jawan was on Tuesday axed to death with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98397</th>\n",
       "      <td>First song from Sonakshi Sinha's 'Noor' titled...</td>\n",
       "      <td>'Uff Yeh', the first song from the Sonakshi Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98398</th>\n",
       "      <td>'The Matrix' film to get a reboot: Reports</td>\n",
       "      <td>Michael B Jordan will reportedly play the lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98399</th>\n",
       "      <td>Snoop Dogg aims gun at clown dressed as Trump ...</td>\n",
       "      <td>The video also shows a TV airing a news confer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98400</th>\n",
       "      <td>Madhesi Morcha withdraws support to Nepalese g...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98401 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "0      upGrad learner switches to career in ML & Al w...   \n",
       "1      Delhi techie wins free food from Swiggy for on...   \n",
       "2      New Zealand end Rohit Sharma-led India's 12-ma...   \n",
       "3      Aegon life iTerm insurance plan helps customer...   \n",
       "4      Have known Hirani for yrs, what if MeToo claim...   \n",
       "...                                                  ...   \n",
       "98396  CRPF jawan axed to death by Maoists in Chhatti...   \n",
       "98397  First song from Sonakshi Sinha's 'Noor' titled...   \n",
       "98398         'The Matrix' film to get a reboot: Reports   \n",
       "98399  Snoop Dogg aims gun at clown dressed as Trump ...   \n",
       "98400  Madhesi Morcha withdraws support to Nepalese g...   \n",
       "\n",
       "                                      extractive_summary  \n",
       "0      upGrad's Online Power Learning has powered 3 l...  \n",
       "1      Users get one CRED coin per rupee of bill paid...  \n",
       "2      The match witnessed India getting all out for ...  \n",
       "3      Also, customers have options to insure against...  \n",
       "4      Speaking about the sexual harassment allegatio...  \n",
       "...                                                  ...  \n",
       "98396  A CRPF jawan was on Tuesday axed to death with...  \n",
       "98397  'Uff Yeh', the first song from the Sonakshi Si...  \n",
       "98398  Michael B Jordan will reportedly play the lead...  \n",
       "98399  The video also shows a TV airing a news confer...  \n",
       "98400                                                     \n",
       "\n",
       "[98401 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df = data.copy()\n",
    "summary_df = summary_df.drop(columns=[\"text\"])\n",
    "summary_df = pd.concat([summary_df, res_ext], axis=1)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추상적 요약 Abstractive Summarization\n",
    "## 1. 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 98401 entries, 0 to 98400\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   headlines  98401 non-null  object\n",
      " 1   text       98401 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# 데이터 개수 파악\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "헤드라인의 중복이 아닌 개수 :  98280\n",
      "중복이 아닌 텍스트 개수 :  98360\n"
     ]
    }
   ],
   "source": [
    "# 데이터 중복이 아닌 데이터 개수 확인\n",
    "print(\"헤드라인의 중복이 아닌 개수 : \", data[\"headlines\"].nunique())\n",
    "print(\"중복이 아닌 텍스트 개수 : \", data[\"text\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 98360 entries, 0 to 98400\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   headlines  98360 non-null  object\n",
      " 1   text       98360 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 텍스트는 98401개이나 중복이 아닌 텍스트가 98,360으로 중복 데이터가 41개 존재\n",
    "# 위 결과 중복된 텍스트가 존재하므로 중복을 제거하자\n",
    "\n",
    "data.drop_duplicates(subset=[\"text\"], inplace=True)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "중복 텍스트를 제거한 결과 헤드라인과 텍스트가 98,360개가 되었다.\n",
    "이번에는 널 데이터를 확인하고 제거하자\n",
    "\"\"\"\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 널처리를 하려고했으나 널데이터가 존재하지 않는다\n",
    "# 다음으로 텍스트 정규화 사전을 준비하자.\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "len(contractions)\n",
    "\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화 사전을 준비하였으니 이번에는 불용어를 확인해보자\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식과 정규화 사전을 이용하여 전처리를 하고, 토큰화를 시키자\n",
    "import re\n",
    "\n",
    "def preprocessing(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"\\([^)]*\\)\",\"\",sentence)\n",
    "    sentence = re.sub('\"', '', sentence)\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")])\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence)\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    sentence = re.sub(\"[m]{2,}\",\"mm\", sentence)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything bought great infact ordered twice third ordered br mother father\n",
      "great way to start the day\n"
     ]
    }
   ],
   "source": [
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(preprocessing(temp_text))\n",
    "print(preprocessing(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = []\n",
    "preprocessed_headlines = []\n",
    "\n",
    "for headline in data[\"headlines\"]:\n",
    "    preprocessed_headlines.append(preprocessing(headline, False))\n",
    "\n",
    "for text in data[\"text\"]:\n",
    "    preprocessed_text.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
