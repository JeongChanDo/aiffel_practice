{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "['song_lyrics/britney-spears.txt', 'song_lyrics/bieber.txt', 'song_lyrics/lin-manuel-miranda.txt', 'song_lyrics/bjork.txt', 'song_lyrics/jimi-hendrix.txt', 'song_lyrics/blink-182.txt', 'song_lyrics/amy-winehouse.txt', 'song_lyrics/radiohead.txt', 'song_lyrics/r-kelly.txt', 'song_lyrics/paul-simon.txt']\n"
     ]
    }
   ],
   "source": [
    "# glob can is used to load files easily\n",
    "import glob\n",
    "import os\n",
    "txt_file_path = \"song_lyrics/*\"\n",
    "txt_lst = glob.glob(txt_file_path)\n",
    "print(len(txt_lst))\n",
    "print(txt_lst[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of raw_corpus :  187088\n",
      "examples : \n",
      " ['They say get ready for the revolution', \"I think it's time we find some sorta solution\"]\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = []\n",
    "\n",
    "# read all txt_lst\n",
    "for txt_file in txt_lst:\n",
    "    #open text file, read, and then add into raw_corpus\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        #read().splitlines() : read all lines and split it by \\n\n",
    "        try:\n",
    "            raw = f.read().splitlines()\n",
    "            #add all lines into list seperately\n",
    "            raw_corpus.extend(raw)\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(\"current txt_file : \", txt_file)\n",
    "            print(e)\n",
    "            \n",
    "\n",
    "print(\"length of raw_corpus : \", len(raw_corpus))\n",
    "print(\"examples : \\n\",raw_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing\n",
    "- basic : delete duplicates and null\n",
    "- natural language processing : replacing special charater/blank, tokenization, word 2 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### comparision raw sentence and preprocessed sentence ###\n",
      "selected sentence :  Settle up and get your rhythm\n",
      "after preprocessing :  <start> settle up and get your rhythm <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def preprocess_sentence(raw_sentence):\n",
    "    low_sentence = raw_sentence.lower().strip()\n",
    "    special_space_sentence = re.sub(r\"([?.!,¿])\",r\"\\1 \", low_sentence)\n",
    "    empty_space_sentence = re.sub(r'[\" \"]+', \" \", special_space_sentence)\n",
    "    all_space_sentence = re.sub(r\"[^a-zA-Z.!,¿]+\",\" \", empty_space_sentence)\n",
    "    striped_sentence = all_space_sentence.strip()\n",
    "    if len(striped_sentence) == 0:\n",
    "        return None\n",
    "    sentence = \"<start> \" + striped_sentence + \" <end>\"\n",
    "    return sentence\n",
    "\n",
    "\n",
    "idx = int(random.random() * 100)\n",
    "selected_sentence = raw_corpus[idx]\n",
    "preprocessed_sentence = preprocess_sentence(selected_sentence)\n",
    "print(\"### comparision raw sentence and preprocessed sentence ###\")\n",
    "print(\"selected sentence : \", selected_sentence)\n",
    "print(\"after preprocessing : \", preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175920\n",
      "### print 5 preprocessed sentences ##\n",
      "['<start> they say get ready for the revolution <end>', '<start> i think it s time we find some sorta solution <end>', '<start> somebody s caught up in the endless pollution <end>', '<start> they need to wake up, stop living illusions i know you need to hear this <end>', '<start> why won t somebody feel this <end>']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    # if sentence is empty, pass\n",
    "    if len(sentence) == 0: continue\n",
    "    preprocessed = preprocess_sentence(sentence)\n",
    "    if preprocessed != None:\n",
    "        corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "print(len(corpus))\n",
    "print(\"### print 5 preprocessed sentences ##\")\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.shape :  (175920, 15)\n",
      "[[   0    0    0 ...    5 2487    3]\n",
      " [   0    0    0 ... 4606 7411    3]\n",
      " [   0    0    0 ... 3468    1    3]\n",
      " ...\n",
      " [   0    0    0 ...    6  880    3]\n",
      " [   9 6293  395 ...   18 1007    3]\n",
      " [   0    0    0 ...    4  804    3]] <keras_preprocessing.text.Tokenizer object at 0x7fd55015ed10>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,\n",
    "        filters='',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    # token이 너무 크면 공백이 많아지므로 최대 길이를 15로 지정\n",
    "    # 마지막 단어가 출력에 가까운게 좋으므로 앞에 패딩을 뭍임\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15,padding=\"pre\")\n",
    "    print(\"tensor.shape : \", tensor.shape)\n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    2   40   69   39  325   25    5 2487\n",
      "     3]\n",
      " [   0    0    0    2    4  122   10   15   74   21  195   93 4606 7411\n",
      "     3]\n",
      " [   0    0    0    0    0    2  267   15  623   33   13    5 3468    1\n",
      "     3]]\n",
      "<class 'dict'>\n",
      "1  :  <unk>\n",
      "2  :  <start>\n",
      "3  :  <end>\n",
      "4  :  i\n",
      "5  :  the\n"
     ]
    }
   ],
   "source": [
    "# print tensor and tokenizer's values\n",
    "print(tensor[:3, :])\n",
    "print(type(tokenizer.index_word))\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \" : \", tokenizer.index_word[idx])\n",
    "    if idx >= 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset split and preparation for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (140736, 14)\n",
      "X_val.shape :  (35184, 14)\n",
      "y_train.shape :  (140736, 14)\n",
      "y_val.shape :  (35184, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\"\"\"\n",
    "preparing dataset\n",
    "- x : idx 0 ~ -2\n",
    "- y : idx 1 ~ -1\n",
    "\n",
    "\"\"\"\n",
    "X = tensor[:,:-1]\n",
    "y = tensor[:,1:]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"X_train.shape : \", X_train.shape)\n",
    "print(\"X_val.shape : \", X_val.shape)\n",
    "print(\"y_train.shape : \", y_train.shape)\n",
    "print(\"y_val.shape : \", y_val.shape)\n",
    "\n",
    "\n",
    "training_size = len(X_train)\n",
    "valiation_size = len(X_val)\n",
    "batch_size = 256\n",
    "steps_per_epochs = training_size//batch_size\n",
    "\n",
    "def get_dataset(source_input, target_input):\n",
    "    buffer_size = len(source_input)\n",
    "    # number of words + 1(<pad> is not included in tokenizer)\n",
    "    vocab_size = tokenizer.num_words + 1\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((source_input, target_input)).shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = get_dataset(X_train, y_train) \n",
    "val_dataset = get_dataset(X_val, y_val)\n",
    "\n",
    "del raw_corpus, corpus, tensor, X,\\\n",
    "        y, X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n",
      "<TakeDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n",
      "tf.Tensor(\n",
      "[[  0   0   0 ...  10  68   6]\n",
      " [  0   0   0 ...  13   5 357]\n",
      " [  0   0   0 ...  43 793 782]\n",
      " ...\n",
      " [  0   2   4 ...  33  17   6]\n",
      " [  0   0   0 ...   2   5 820]\n",
      " [  0   0   0 ...  13   5 411]], shape=(256, 14), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  0   0   0 ...  68   6   3]\n",
      " [  0   0   0 ...   5 357   3]\n",
      " [  0   0   0 ... 793 782   3]\n",
      " ...\n",
      " [  2   4 195 ...  17   6   3]\n",
      " [  0   0   0 ...   5 820   3]\n",
      " [  0   0   0 ...   5 411   3]], shape=(256, 14), dtype=int32)\n",
      "549\n",
      "549\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.take(1))\n",
    "print(val_dataset.take(1))\n",
    "for sources, targets in train_dataset.take(1):\n",
    "    print(sources)\n",
    "    print(targets)\n",
    "\n",
    "cnt = 0\n",
    "for x in train_dataset:\n",
    "    cnt +=1\n",
    "print(cnt)\n",
    "print(training_size//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# design dnn model and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class LyricGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(LyricGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        #return_sequences means return same length of seqences with input's length\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "embedding_size = 128\n",
    "hidden_size = 128\n",
    "model = LyricGenerator(tokenizer.num_words+1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "549/549 [==============================] - ETA: 0s - loss: 6.2794 - accuracy: 0.3207\n",
      "Epoch 00001: val_loss improved from inf to 5.98853, saving model to /home/aiffel/github/aiffel_practice/EXPLORATION06/checkpoint/lyric_model\n",
      "549/549 [==============================] - 29s 54ms/step - loss: 6.2794 - accuracy: 0.3207 - val_loss: 5.9885 - val_accuracy: 0.3218\n",
      "Epoch 2/3\n",
      "549/549 [==============================] - ETA: 0s - loss: 5.7527 - accuracy: 0.3207\n",
      "Epoch 00002: val_loss improved from 5.98853 to 5.54380, saving model to /home/aiffel/github/aiffel_practice/EXPLORATION06/checkpoint/lyric_model\n",
      "549/549 [==============================] - 27s 49ms/step - loss: 5.7527 - accuracy: 0.3207 - val_loss: 5.5438 - val_accuracy: 0.3217\n",
      "Epoch 3/3\n",
      "548/549 [============================>.] - ETA: 0s - loss: 5.3794 - accuracy: 0.3208\n",
      "Epoch 00003: val_loss improved from 5.54380 to 5.24022, saving model to /home/aiffel/github/aiffel_practice/EXPLORATION06/checkpoint/lyric_model\n",
      "549/549 [==============================] - 27s 49ms/step - loss: 5.3794 - accuracy: 0.3207 - val_loss: 5.2402 - val_accuracy: 0.3218\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True,\n",
    "        reduction=\"none\"\n",
    "    )\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "checkpoint_dir = os.getenv(\"HOME\")+\"/github/aiffel_practice/EXPLORATION06/checkpoint/lyric_model\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_dir,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    mode=\"auto\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(train_dataset, epochs=epochs,\n",
    "        steps_per_epoch= training_size//batch_size,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=valiation_size//batch_size,\n",
    "        callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate lyrics test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 2  4 56  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]], shape=(1, 20), dtype=int64)\n",
      "2 \n",
      "4 <start> \n",
      "56 <start> i \n",
      "0 <start> i want \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-56a55cc9a867>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mgenerate_lyrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<start> I want\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-56a55cc9a867>\u001b[0m in \u001b[0;36mgenerate_lyrics\u001b[0;34m(model, tokenizer, init_sentence, max_len)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mgenerated\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def generate_lyrics(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predicted_word = tf.argmax(tf.nn.softmax(predict,axis = -1), axis=-1)[:, -1]\n",
    "        test_tensor = tf.concat([test_tensor,\n",
    "                                tf.expand_dims(predicted_word, axis=0)], axis=-1)\n",
    "        if predicted_word.numpy()[0] == end_token:break\n",
    "        if test_tensor.shape[1] >= max_len:break\n",
    "        \n",
    "    generated = \"\"\n",
    "    print(test_tensor)\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        print(word_index, generated)\n",
    "        generated += tokenizer.index_word[word_index] +\" \"\n",
    "    return generated\n",
    "\n",
    "generate_lyrics(model, tokenizer, init_sentence=\"<start> I want\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
