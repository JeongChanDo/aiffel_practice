{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "['song_lyrics/britney-spears.txt', 'song_lyrics/bieber.txt', 'song_lyrics/lin-manuel-miranda.txt', 'song_lyrics/bjork.txt', 'song_lyrics/jimi-hendrix.txt', 'song_lyrics/blink-182.txt', 'song_lyrics/amy-winehouse.txt', 'song_lyrics/radiohead.txt', 'song_lyrics/r-kelly.txt', 'song_lyrics/paul-simon.txt']\n"
     ]
    }
   ],
   "source": [
    "# glob can is used to load files easily\n",
    "import glob\n",
    "txt_file_path = \"song_lyrics/*\"\n",
    "txt_lst = glob.glob(txt_file_path)\n",
    "print(len(txt_lst))\n",
    "print(txt_lst[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of raw_corpus :  187088\n",
      "examples : \n",
      " ['They say get ready for the revolution', \"I think it's time we find some sorta solution\"]\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = []\n",
    "\n",
    "# read all txt_lst\n",
    "for txt_file in txt_lst:\n",
    "    #open text file, read, and then add into raw_corpus\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        #read().splitlines() : read all lines and split it by \\n\n",
    "        raw = f.read().splitlines()\n",
    "        #add all lines into list seperately\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"length of raw_corpus : \", len(raw_corpus))\n",
    "print(\"examples : \\n\",raw_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing\n",
    "- basic : delete duplicates and null\n",
    "- natural language processing : replacing special charater/blank, tokenization, word 2 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### comparision raw sentence and preprocessed sentence ###\n",
      "selected sentence :  It's like a competition\n",
      "after preprocessing :  <start>it s like a competition<end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def preprocess_sentence(raw_sentence):\n",
    "    low_sentence = raw_sentence.lower().strip()\n",
    "    special_space_sentence = re.sub(r\"([?.!,¿])\",r\"\\1 \", low_sentence)\n",
    "    empty_space_sentence = re.sub(r'[\" \"]+', \" \", special_space_sentence)\n",
    "    all_space_sentence = re.sub(r\"[^a-zA-Z.!,¿]+\",\" \", empty_space_sentence)\n",
    "    striped_sentence = all_space_sentence.strip()\n",
    "    sentence = \"<start>\" + striped_sentence + \"<end>\"\n",
    "    return sentence\n",
    "\n",
    "\n",
    "idx = int(random.random() * 100)\n",
    "selected_sentence = raw_corpus[idx]\n",
    "preprocessed_sentence = preprocess_sentence(selected_sentence)\n",
    "print(\"### comparision raw sentence and preprocessed sentence ###\")\n",
    "print(\"selected sentence : \", selected_sentence)\n",
    "print(\"after preprocessing : \", preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### print 5 preprocessed sentences ##\n",
      "['<start>they say get ready for the revolution<end>', '<start>i think it s time we find some sorta solution<end>', '<start>somebody s caught up in the endless pollution<end>', '<start>they need to wake up, stop living illusions i know you need to hear this<end>', '<start>why won t somebody feel this<end>']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    # if sentence is empty, pass\n",
    "    if len(sentence) == 0: continue\n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "print(\"### print 5 preprocessed sentences ##\")\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.shape :  (175986, 15)\n",
      "[[   0    0    0 ...   26    2 8032]\n",
      " [   0    0    0 ...  100 7014 9457]\n",
      " [   0    0    0 ...    2 4331    1]\n",
      " ...\n",
      " [   0    0    0 ...  423    4 3485]\n",
      " [1820    5 9215 ...  446   17 1745]\n",
      " [   0    0    0 ...  149    3 1895]] <keras_preprocessing.text.Tokenizer object at 0x7f732af3a410>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=10000,\n",
    "        filters='',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    # token이 너무 크면 공백이 많아지므로 최대 길이를 15로 지정\n",
    "    # 마지막 단어가 출력에 가까운게 좋으므로 앞에 패딩을 뭍임\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15,padding=\"pre\")\n",
    "    print(\"tensor.shape : \", tensor.shape)\n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0  110   74   34  465   26    2\n",
      "  8032]\n",
      " [   0    0    0    0    0    7  123   11   12   94   24  217  100 7014\n",
      "  9457]\n",
      " [   0    0    0    0    0    0    0 1113   12  811   35   13    2 4331\n",
      "     1]]\n",
      "<class 'dict'>\n",
      "1  :  <unk>\n",
      "2  :  the\n",
      "3  :  i\n",
      "4  :  you\n",
      "5  :  a\n"
     ]
    }
   ],
   "source": [
    "# print tensor and tokenizer's values\n",
    "print(tensor[:3, :])\n",
    "print(type(tokenizer.index_word))\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \" : \", tokenizer.index_word[idx])\n",
    "    if idx >= 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0 110  74  34 465  26   2]\n",
      "[   0    0    0    0    0    0    0  110   74   34  465   26    2 8032]\n",
      "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "preparing dataset\n",
    "- source input : idx 0 ~ -2\n",
    "- target input : idx 1 ~ -1\n",
    "\n",
    "\"\"\"\n",
    "source_input = tensor[:,:-1]\n",
    "target_input = tensor[:,1:]\n",
    "print(source_input[0])\n",
    "print(target_input[0])\n",
    "\n",
    "\n",
    "buffer_size = len(source_input)\n",
    "batch_size = 256\n",
    "steps_per_epochs = buffer_size//batch_size\n",
    "# number of words + 1(<pad> is not included in tokenizer)\n",
    "vocab_size = tokenizer.num_words + 1\n",
    "dataset = tf.data.Dataset.from_tensor_slices((source_input, target_input)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "del raw_corpus, corpus, tensor, source_input, target_input\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
